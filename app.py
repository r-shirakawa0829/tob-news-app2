import streamlit as st
import feedparser
import pandas as pd
import datetime
import os
import urllib.parse

# --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼šå…¬å…±ãƒ»æµ·å¤–ã‚’é™¤å¤– ---
def is_target_company(title, summary):
    text = (title + summary).lower()
    
    # 1. å®˜å…¬åºãƒ»æ•™è‚²æ©Ÿé–¢ãªã©ã®é™¤å¤–
    exclude_keywords = [
        "å¤§å­¦", "æ•™æˆ", "ç ”ç©¶å®¤", "å­¦ç”Ÿ", "é«˜æ ¡", "å­¦æ ¡",
        "å¸‚å½¹æ‰€", "çœŒåº", "éƒ½åº", "åŒºå½¹æ‰€", "å½¹å ´", "çœ", "åº", "å†…é–£",
        "è‡ªæ²»ä½“", "è¦³å…‰å”ä¼š", "ãƒ•ã‚§ã‚¹ãƒ†ã‚£ãƒãƒ«", "ç¥­ã‚Š", "å…¬å‹Ÿ", "å…¥æœ­",
        "æ©Ÿæ§‹", "è²¡å›£", "é€£åˆä¼š", "å”è­°ä¼š", "è­¦å¯Ÿ", "æ¶ˆé˜²"
    ]
    if any(k in text for k in exclude_keywords):
        return False
        
    # 2. ã€NEWã€‘æµ·å¤–ä¼æ¥­ã®é™¤å¤–ï¼ˆå›½å†…ä¼æ¥­ã«çµã‚‹ï¼‰
    # ã€Œç¾åœ°æ™‚é–“ã€ã€Œãƒ‰ãƒ«ã€ã€Œå…ƒã€ã€Œã‚¦ã‚©ãƒ³ã€ãªã©æµ·å¤–ç‰¹æœ‰ã®å˜èªã‚’é™¤å¤–
    foreign_keywords = [
        "ç¾åœ°æ™‚é–“", "ãƒ‰ãƒ«", "ãƒ¦ãƒ¼ãƒ­", "å…ƒ", "ã‚¦ã‚©ãƒ³", 
        "ç±³å›½", "ä¸­å›½", "æ¬§å·", "æµ·å¤–æœ¬ç¤¾", "æ—¥æœ¬æ³•äºº", "æ”¯ç¤¾"
    ]
    # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã«ã€Œæµ·å¤–ã€ãŒå¼·ãå‡ºã‚‹ã‚‚ã®ã‚‚é¿ã‘ã‚‹ï¼ˆæµ·å¤–é€²å‡ºãªã‚‰OKã ãŒã€æµ·å¤–ä¼æ¥­ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯NGï¼‰
    if any(k in text for k in foreign_keywords):
        return False
        
    return True

# --- â˜…ãƒ“ã‚¸ãƒã‚¹ã‚¿ãƒ³ã‚¯ãƒ»ãƒãƒƒãƒåº¦åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ç‰¹åŒ–ç‰ˆï¼‰â˜… ---
def analyze_business_tank_fit(title, summary):
    text = (title + summary).lower()
    tags = []
    score = 0

    # 1. ã€Crownã€‘è²©è·¯æ‹¡å¤§ãƒ»è²©å£²åº—å‹Ÿé›†ï¼ˆæœ€å„ªå…ˆï¼‰
    # ã“ã“ã¯å¤‰ã‚ã‚‰ãšæœ€å¼·ï¼ˆ+10ç‚¹ï¼‰
    crown_keywords = [
        "è²©è·¯æ‹¡å¤§", "è²©è·¯é–‹æ‹“", "è²©å£²åº—å‹Ÿé›†", "è²©å£²åº— å‹Ÿé›†", 
        "ä»£ç†åº—å‹Ÿé›†", "ä»£ç†åº— å‹Ÿé›†", "ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼å‹Ÿé›†"
    ]
    for k in crown_keywords:
        if k in text:
            tags.append(f"ğŸ‘‘{k}")
            score += 10

    # 2. ã€Startupã€‘æˆé•·ãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã®å‹•ãï¼ˆæ¿€ã‚¢ãƒ„ï¼‰
    # è³‡é‡‘èª¿é”ã‚„ãƒªãƒªãƒ¼ã‚¹ã¯ã€Œã“ã‚Œã‹ã‚‰ä¼¸ã³ã‚‹ã€è¨¼æ‹ ãªã®ã§é«˜å¾—ç‚¹ï¼ˆ+5ç‚¹ï¼‰
    startup_keywords = [
        "è³‡é‡‘èª¿é”", "ç¬¬ä¸‰è€…å‰²å½“", "ã‚·ãƒªãƒ¼ã‚ºa", "ã‚·ãƒªãƒ¼ã‚ºb", "j-kiss",
        "æ–°ã‚µãƒ¼ãƒ“ã‚¹", "æ–°å•†å“", "ãƒ—ãƒ¬ãƒªãƒªãƒ¼ã‚¹", "ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹", 
        "ãƒ­ãƒ¼ãƒ³ãƒ", "æä¾›é–‹å§‹", "ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—", "ãƒ™ãƒ³ãƒãƒ£ãƒ¼", "è¨­ç«‹"
    ]
    for k in startup_keywords:
        if k in text:
            tags.append(f"ğŸ”¥{k}")
            score += 5

    # 3. ã€Partnershipã€‘ææºï¼ˆå„ªå…ˆåº¦ãƒ€ã‚¦ãƒ³ï¼‰
    # å¤§æ‰‹ã¨ã®ææºã¯ä¸è¦ã¨ã®ã“ã¨ãªã®ã§ã€ç‚¹æ•°ã‚’æ§ãˆã‚ã«ï¼ˆ+1ç‚¹ï¼‰
    partner_keywords = [
        "ææº", "å…±åŒç ”ç©¶", "å…±åŒé–‹ç™º", "å®Ÿè¨¼å®Ÿé¨“", "å”æ¥­", 
        "ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹", "ã‚ªãƒ¼ãƒ—ãƒ³ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³", "OEM"
    ]
    for k in partner_keywords:
        if k in text:
            tags.append(f"ğŸ¤{k}")
            score += 1

    # 4. ã€Changeã€‘å¤‰åŒ–ã®äºˆå…†ï¼ˆ+1ç‚¹ï¼‰
    change_keywords = [
        "æ–°è¦äº‹æ¥­", "äº‹æ¥­æ‹¡å¤§", "ç¤¾é•·å°±ä»»", "æ–°ä½“åˆ¶", 
        "çµŒå–¶è¨ˆç”»", "åˆ·æ–°", "DX", "IPO", "é»’å­—åŒ–"
    ]
    for k in change_keywords:
        if k in text:
            tags.append(f"ğŸ“ˆ{k}")
            score += 1

    # 5. ã€Penaltyã€‘å¤§æ‰‹ãƒ»æœ‰åä¼æ¥­ã®æ¸›ç‚¹ï¼ˆ-10ç‚¹ï¼‰
    # å¤§æ‰‹ã¯ã‚¬ãƒ¼ãƒ‰ãŒå›ºã„ã®ã§ãƒªã‚¹ãƒˆã®ä¸‹ã¸
    big_company_keywords = [
        "å¤§æ‰‹", "æœ€å¤§æ‰‹", "æ¥­ç•Œãƒˆãƒƒãƒ—", "æ±è¨¼ãƒ—ãƒ©ã‚¤ãƒ ", "è€èˆ—", 
        "æœ‰å", "ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹", "ã‚°ãƒ«ãƒ¼ãƒ—"
    ]
    for k in big_company_keywords:
        if k in text:
            score -= 10
            tags.append(f"ğŸ¢{k}(å¤§æ‰‹)")

    return score, list(set(tags))

def fetch_all_sources():
    # æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼šã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ»æˆé•·ä¼æ¥­ç‹™ã„æ’ƒã¡
    feeds = [
        "https://prtimes.jp/index.rdf",
        # è³‡é‡‘èª¿é”ãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—æƒ…å ±ã‚’å¼·åŒ–
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("è³‡é‡‘èª¿é” å®Ÿæ–½ ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ–°ã‚µãƒ¼ãƒ“ã‚¹ é–‹å§‹ æ³•äºº") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("è²©å£²åº—å‹Ÿé›†") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("ä»£ç†åº—å‹Ÿé›†") + "&hl=ja&gl=JP&ceid=JP:ja",
        # ã€Œè²©è·¯æ‹¡å¤§ã€ãã®ã‚‚ã®ã‚’ç‹™ã†
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("è²©è·¯æ‹¡å¤§ ç›®æŒ‡ã™") + "&hl=ja&gl=JP&ceid=JP:ja"
    ]
    
    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    new_entries = []
    
    for url in feeds:
        feed = feedparser.parse(url)
        for entry in feed
