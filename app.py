import streamlit as st
import feedparser
import pandas as pd
import datetime
import os
import urllib.parse

def analyze_growth_company(title, summary):
    text = (title + summary).lower()
    growth_keywords = ["æ¡ç”¨", "å‹Ÿé›†", "ç§»è»¢", "å¢—åºŠ", "æ–°æ‹ ç‚¹", "æµ·å¤–å±•é–‹", "æ–°è¦äº‹æ¥­", "è³‡é‡‘èª¿é”", "ææº", "å°å…¥", "é–‹å§‹", "ãƒ­ãƒ¼ãƒ³ãƒ", "å­ä¼šç¤¾"]
    biz_keywords = ["æ³•äºº", "ä¼æ¥­", "b2b", "saas", "dx", "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³", "oem", "å¸", "åŠ ç›Ÿ", "fc"]
    return any(k in text for k in growth_keywords) and any(k in text for k in biz_keywords)

def fetch_all_sources():
    feeds = [
        "https://prtimes.jp/index.rdf",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ¡ç”¨å¼·åŒ– ä¼æ¥­") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ–°äº‹æ¥­ é–‹å§‹") + "&hl=ja&gl=JP&ceid=JP:ja"
    ]
    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    new_entries = []
    for url in feeds:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if analyze_growth_company(entry.title, entry.summary):
                title_clean = entry.title.replace("ã€", " ").replace("ã€‘", " ").replace("ã€Œ", " ").replace("ã€", " ")
                company = title_clean.split("ãŒ")[0].split("ã®")[0].strip()[:20]
                new_entries.append([today_str, now.strftime("%H:%M"), company, entry.title, entry.link])
    db_file = "news_database.csv"
    if new_entries:
        df_new = pd.DataFrame(new_entries, columns=["date", "time", "company", "title", "url"])
        if os.path.exists(db_file):
            df_old = pd.read_csv(db_file)
            df_final = pd.concat([df_new, df_old]).drop_duplicates(subset=["url"], keep="first")
        else:
            df_final = df_new
        df_final.sort_values(by=["date", "time"], ascending=False).to_csv(db_file, index=False, encoding="utf_8_sig")
    return len(new_entries)

st.set_page_config(page_title="Growth Company Hub", layout="wide")
st.title("ğŸš€ æˆé•·ä¼æ¥­ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒªã‚¹ãƒˆ")

if st.button("ğŸ”„ æœ€æ–°æƒ…å ±ã‚’å–å¾—"):
    count = fetch_all_sources()
    st.success(f"{count}ä»¶æ›´æ–°ã—ã¾ã—ãŸ")
    st.rerun()

db_file = "news_database.csv"
if os.path.exists(db_file):
    df = pd.read_csv(db_file)
    for d in df["date"].unique():
        st.markdown(f"#### ğŸ“… {d}")
        day_df = df[df["date"] == d]
        for _, row in day_df.iterrows():
            st.markdown(f"âœ… **{row['company']}** [{row['title']}]({row['url']})")
